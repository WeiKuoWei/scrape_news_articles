{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "News article categorization script - Crime focus\n",
    "This script analyzes news articles and categorizes them based on crime-related keywords\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "SITE_LIST = [\"foxnews\"] # , \"foxnews\"\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"..\", \"..\", \"data\")\n",
    "REFERENCE_DATA_PATH = f\"{DATA_DIR}/News_Category_Dataset_v3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model for text processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_file(file_path):\n",
    "    \"\"\"Load a JSONL file into a pandas DataFrame\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text for analysis\"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
    "    text = ''.join(c for c in text if not c.isdigit())\n",
    "    # Remove stop words\n",
    "    return \" \".join(word for word in text.split() if word not in STOP_WORDS)\n",
    "\n",
    "def extract_crime_keywords():\n",
    "    \"\"\"Extract top crime-related keywords from reference dataset\"\"\"\n",
    "    print(\"Extracting crime keywords from reference dataset...\")\n",
    "    \n",
    "    # Load reference dataset\n",
    "    df_ref = load_jsonl_file(REFERENCE_DATA_PATH)\n",
    "    \n",
    "    # Filter for crime category and preprocess\n",
    "    df_crime = df_ref[df_ref[\"category\"] == \"CRIME\"].copy()\n",
    "    df_crime[\"headline\"] = df_crime[\"headline\"].apply(preprocess_text)\n",
    "    df_crime[\"short_description\"] = df_crime[\"short_description\"].apply(preprocess_text)\n",
    "    \n",
    "    # Count word frequencies\n",
    "    crime_word_counter = Counter()\n",
    "    for _, row in df_crime.iterrows():\n",
    "        crime_word_counter.update(row[\"headline\"].split())\n",
    "        crime_word_counter.update(row[\"short_description\"].split())\n",
    "    \n",
    "    # Non-crime-related words to filter out\n",
    "    non_related_words = {\n",
    "        'new', 'florida', 'texas', 'nyc', 'california', 'says', 'idaho', 'chicago',\n",
    "        'georgia', 'carolina', 'york', 'city', 'video', 'car', 'home', 'years', 'family',\n",
    "        'los', 'mom', 'angeles', 'north', 'men', 'state', 'judge', 'officials', 'house',\n",
    "        'near', 'people', 'sex', 'virginia', 'university', 'court', 'philadelphia', 'fire',\n",
    "        'subway', 'nypd', 'arizona', 'washington', 'women', 'school', 'teen',\n",
    "        'mother', 'woman', 'shows', 'girl', 'driver', 'student', 'child', 'search', 'leaves', 'found',\n",
    "        'man', 'san', 'high', 'ohio', 'photo'\n",
    "    }\n",
    "    \n",
    "    # Filter and get top crime words\n",
    "    top_crime_words = [word for word, _ in crime_word_counter.most_common(200) \n",
    "                     if word not in non_related_words][:100]\n",
    "    \n",
    "    print(f\"Extracted {len(top_crime_words)} crime keywords\")\n",
    "    print(f\"Top 10 crime keywords: {', '.join(top_crime_words[:10])}\")\n",
    "    return top_crime_words\n",
    "\n",
    "def categorize_articles(site_name, crime_keywords):\n",
    "    \"\"\"Categorize articles for a specific site based on crime keywords\"\"\"\n",
    "    print(f\"Categorizing articles for {site_name}...\")\n",
    "    \n",
    "    # Load articles\n",
    "    file_path = os.path.join(DATA_DIR, site_name, \"backup\", \"articles_cleaned.jsonl\")\n",
    "    try:\n",
    "        df = load_jsonl_file(file_path)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Preprocess text fields\n",
    "    df['title_processed'] = df['title'].apply(preprocess_text)\n",
    "    df['description_processed'] = df['description'].apply(preprocess_text)\n",
    "    \n",
    "    # Check if articles contain crime keywords\n",
    "    def is_crime_article(row):\n",
    "        \"\"\"Check if an article contains enough crime keywords to be categorized as crime\"\"\"\n",
    "        title_words = set(row[\"title_processed\"].split())\n",
    "        desc_words = set(row[\"description_processed\"].split())\n",
    "        all_words = title_words.union(desc_words)\n",
    "        \n",
    "        # Count crime keywords\n",
    "        crime_word_count = sum(1 for word in all_words if word in crime_keywords)\n",
    "        return crime_word_count >= 3  # Require at least 3 crime keywords\n",
    "    \n",
    "    # Apply categorization\n",
    "    df[\"category\"] = \"Other\"\n",
    "    df.loc[df.apply(is_crime_article, axis=1), \"category\"] = \"Crime\"\n",
    "    \n",
    "    # Remove processing columns\n",
    "    df.drop(columns=[\"title_processed\", \"description_processed\"], inplace=True)\n",
    "    \n",
    "    # Save categorized articles\n",
    "    output_path = os.path.join(DATA_DIR, site_name, \"articles_categorized.json\") \n",
    "    df.to_json(output_path, orient='records', lines=True)\n",
    "    \n",
    "    crime_count = len(df[df[\"category\"] == \"Crime\"])\n",
    "    total_count = len(df)\n",
    "    print(f\"Categorized {total_count} articles for {site_name}: {crime_count} crime articles ({crime_count/total_count:.1%})\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crime article categorization...\n",
      "Extracting crime keywords from reference dataset...\n",
      "Extracted 100 crime keywords\n",
      "Top 10 crime keywords: police, shooting, said, yearold, allegedly, suspect, killed, shot, death, accused\n",
      "Categorizing articles for foxnews...\n",
      "File not found: /Users/weikuo/Documents/github-repositories/scrape_news_articles/src/preprocess/../../data/foxnews/backup/articles_cleaned.jsonl\n",
      "Categorization complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function to run the categorization process\"\"\"\n",
    "print(\"Starting crime article categorization...\")\n",
    "\n",
    "# Extract crime keywords from reference dataset\n",
    "crime_keywords = extract_crime_keywords()\n",
    "\n",
    "# Process each site\n",
    "for site in SITE_LIST:\n",
    "    categorize_articles(site, crime_keywords)\n",
    "\n",
    "print(\"Categorization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
